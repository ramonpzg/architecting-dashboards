{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an ML Dashboard App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Overview\n",
    "2. Tools\n",
    "3. ML Model\n",
    "5. ML Microservices\n",
    "    - Music Generator\n",
    "    - Visualizations\n",
    "    - Audio Effects\n",
    "    - Midi Representations\n",
    "6. Front- and Back-End\n",
    "7. Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Microservices architectures are like the building blocks of many modern applications, including dashboards, \n",
    "hence, in this section, we will be creating a live dashboard with a microservices architecture. We will have a \n",
    "model serving requests from one service and the main dashboard in another. In addition, we will have another \n",
    "service updating music to our heart's content.\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MLServer\n",
    "- Gradio\n",
    "- Transformers\n",
    "- matplotlib\n",
    "- pedalboard\n",
    "- librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, MusicgenForConditionalGeneration\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"facebook/musicgen-small\")\n",
    "model     = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(\n",
    "    text=[\"classic music with fast tempo, violin sounds, and guitar solo ending\"],\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = model.config.audio_encoder.sampling_rate\n",
    "sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìàüëÄ üëå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_values.shape[2] / 32000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_values[0, 0].numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_values[0].numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_values[0].numpy(), rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add even more effects with other libraries like librosa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.effects.hpss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_har, y_per = librosa.effects.hpss(audio_values[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(y_har, rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other libraries that give us further control over the knobs we can tune in our songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pedalboard import Pedalboard, Distortion, Delay, Reverb\n",
    "from pedalboard.io import AudioFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with AudioFile(\"live/05mUf9x3V3RIqafuY4H54E.mp3\", \"r\") as f:\n",
    "    song = f.read(f.frames)\n",
    "    sr = f.samplerate\n",
    "song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(song, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board = Pedalboard([\n",
    "    Distortion(drive_db=10),\n",
    "    Delay(delay_seconds=1, feedback=0.1, mix=0.1),\n",
    "    Reverb(room_size=0.10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_song = board(song, sample_rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(new_song, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save our creations.\n",
    "\n",
    "```sh\n",
    "mkdir live/music\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_song.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with AudioFile(\"live/new_notes.mp3\", \"w\", samplerate=sr, num_channels=1) as f:\n",
    "    f.write(new_song[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we did like that last song a lot, we can also create a new one with the tunes we likes as the base for the new song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_values[0].numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(\n",
    "    audio=audio_values[0].numpy()[0],\n",
    "    sampling_rate=sampling_rate,\n",
    "    text=[\"80s blues track with violin notes\"],\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_values[0].numpy(), rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ML Microservices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to get our service up and running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 MusicGen Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with the imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile live/servers/ml_model/ml_service.py\n",
    "\n",
    "from mlserver import MLServer, Settings, ModelSettings, MLModel\n",
    "from mlserver.codecs import decode_args\n",
    "\n",
    "from transformers import AutoProcessor, MusicgenForConditionalGeneration\n",
    "import numpy as np\n",
    "\n",
    "from typing import List\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll append our main class to our server file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a live/servers/ml_model/ml_service.py\n",
    "\n",
    "MUSICGEN = \"facebook/musicgen-small\"\n",
    "\n",
    "class MusicGenServer(MLModel):\n",
    "    async def load(self):\n",
    "        self.processor = AutoProcessor.from_pretrained(MUSICGEN)\n",
    "        self.model     = MusicgenForConditionalGeneration.from_pretrained(MUSICGEN)\n",
    "\n",
    "    @decode_args\n",
    "    async def predict(self, text: List[str], guidance_scale: np.ndarray, max_new_tokens: np.ndarray) -> np.ndarray:\n",
    "        inputs       = self.processor(text=text, padding=True, return_tensors=\"pt\")\n",
    "        audio_values = self.model.generate(\n",
    "            **inputs, do_sample=True, guidance_scale=guidance_scale[0][0], max_new_tokens=max_new_tokens[0][0]\n",
    "        ).numpy()\n",
    "        return audio_values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we'll need the settings and asyncio to run our app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a live/servers/ml_model/ml_service.py\n",
    "\n",
    "async def main():\n",
    "    settings = Settings(debug=True)\n",
    "    my_server = MLServer(settings=settings)\n",
    "    musicgen_generator = ModelSettings(name='musicgen_model', implementation=MusicGenServer)\n",
    "    await my_server.start(models_settings=[musicgen_generator])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run our model with:\n",
    "\n",
    "```sh\n",
    "python servers/ml_model/ml_service.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, MLServer will provide you with the following ports:\n",
    "- https\n",
    "- grpc\n",
    "- metrics\n",
    "\n",
    "We can test that our server is working as intended with the following requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlserver.codecs import StringCodec, NumpyCodec\n",
    "import numpy as np\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_text   = StringCodec.encode_input(name='text', payload=['slow bachata with violin sounds'], use_bytes=False)\n",
    "parameter_1 = NumpyCodec.encode_input(name='guidance_scale', payload=np.array([5]))\n",
    "parameter_2 = NumpyCodec.encode_input(name='max_new_tokens', payload=np.array([280]))\n",
    "print(song_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parameter_1.dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine the as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_request = {\n",
    "    'inputs': [\n",
    "        StringCodec.encode_input(name='text', payload=['slow bachata with violin sounds'], use_bytes=False).dict(),\n",
    "        NumpyCodec.encode_input(name='guidance_scale', payload=np.array([5])).dict(),\n",
    "        NumpyCodec.encode_input(name='max_new_tokens', payload=np.array([200])).dict()\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_request = {\n",
    "    'inputs': [song_text.dict(), parameter_1.dict(), parameter_2.dict()]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the resul will be the same.\n",
    "\n",
    "Here is the endpoint we'll use for our request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"http://localhost:8080/v2/models/musicgen_model/infer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above seems a bit different than the endpoints of previous sections, that's because \n",
    "MLServer uses the Open Inference Protocol (OIP) or V2 Inference Protocol to communicate.\n",
    "\n",
    "The OIP is a standard created in a effort to standardize the way in which machine learning \n",
    "microservices communicate with each other. It has been adopted by companies such as NVIDIA, \n",
    "Google, and others, and it is the standard used by tools such as Seldon Core and KServe. To \n",
    "learn more about the OIP, see the [OIP website](https://github.com/SeldonIO/seldon-core/blob/master/docs/protocol.md).\n",
    "\n",
    "Now, back to making predictions.\n",
    "\n",
    "**Note:** This could take a few seconds yo run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = requests.post(endpoint, json=input_request)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at what the model and server will return to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to evaluate the quality of the song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(result.json()['outputs'][0]['data'], rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Test the endpoint by sending a few requests to it with your best description of a song.\n",
    "2. Look at the part of our model that's making the inference and pick a parameter that has \n",
    "not been added to the model yet.\n",
    "3. Stop the server, add the parameter to the model, and start the server again.\n",
    "4. Try out the same songs you tested before against the new parameters of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to visualize sound, and, for our purposes, we will create a waveform and \n",
    "a spectogram for each of our newest songs.\n",
    "\n",
    "**Waveform**\n",
    "> A waveform plot is a visual representation of sound waves over time. Imagine you're looking at a graph where the horizontal axis shows time, and the vertical axis represents the intensity or amplitude of the sound. It looks like a bunch of wavy lines. Now, why is this useful? Well, it helps us \"see\" sound. When you talk, sing, or play music, these waves represent the vibrations in the air. By studying these waveforms, scientists, musicians, and engineers can understand the characteristics of sound, like pitch and volume. It's like looking at a musical recipe; you can see how loud or soft the different parts of a song are, just like you can see the ingredients and quantities in a recipe. So, waveform plots are like musical blueprints, helping us visualize and understand the language of sound!\n",
    "\n",
    "**Spectogram**\n",
    "> A spectogram is a visual representation of sound waveforms. Imagine you're looking at a graph where the horizontal axis shows frequency, and the vertical axis represents the amplitude or intensity of the sound. It looks like a bunch of wavy lines. Now, why is this useful? Well, it helps us \"see\" sound. When you talk, sing, or play music, these waveforms represent the vibrations in the air.\n",
    "\n",
    "To create our visualizations, we will use `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_waveform(waveform, sample_rate):\n",
    "    # waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = np.arange(0, num_frames) / sample_rate\n",
    "\n",
    "    figure = Figure() \n",
    "    axes = figure.subplots(num_channels, 1)\n",
    "    if num_channels == 1:\n",
    "        axes = [axes]\n",
    "    for c in range(num_channels):\n",
    "        axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
    "        axes[c].grid(True)\n",
    "        if num_channels > 1:\n",
    "            axes[c].set_ylabel(f\"Channel {c+1}\")\n",
    "    figure.suptitle(\"Waveform\")\n",
    "    plt.show()\n",
    "    return figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_specgram(waveform, sample_rate, title=\"Spectrogram\"):\n",
    "    # waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    figure = Figure() \n",
    "    axes = figure.subplots(num_channels, 1)\n",
    "    # figure, axes = plt.subplots(num_channels, 1)\n",
    "    if num_channels == 1:\n",
    "        axes = [axes]\n",
    "    for c in range(num_channels):\n",
    "        axes[c].specgram(waveform[c], Fs=sample_rate)\n",
    "        if num_channels > 1:\n",
    "            axes[c].set_ylabel(f\"Channel {c+1}\")\n",
    "    figure.suptitle(title)\n",
    "    plt.show()\n",
    "    return figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([result.json()['outputs'][0]['data']]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_specgram(np.array([result.json()['outputs'][0]['data']]), sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_waveform(np.array([result.json()['outputs'][0]['data']]), sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now add these to a plotting file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile live/src/plotting.py\n",
    "\n",
    "from matplotlib.figure import Figure\n",
    "import matplotlib.pyplot as plt\n",
    "from pedalboard.io import AudioFile\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a live/src/plotting.py\n",
    "\n",
    "def get_latest_file():\n",
    "    files =  glob(\"./music/*.mp3\")\n",
    "    latest_file = max(files, key=os.path.getctime)\n",
    "    with AudioFile(latest_file, \"r\") as f:\n",
    "        waveform = f.read(f.frames)\n",
    "        sample_rate = f.samplerate\n",
    "    return waveform, sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a live/src/plotting.py\n",
    "\n",
    "def make_spectogram():\n",
    "    waveform, sample_rate = get_latest_file()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    with plt.xkcd():\n",
    "        figure = Figure() \n",
    "        axes = figure.subplots(num_channels, 1)\n",
    "        if num_channels == 1:\n",
    "            axes = [axes]\n",
    "        for c in range(num_channels):\n",
    "            axes[c].specgram(waveform[c], Fs=sample_rate)\n",
    "            if num_channels > 1:\n",
    "                axes[c].set_ylabel(f\"Channel {c+1}\")\n",
    "        figure.suptitle(\"Spectrogram\")\n",
    "    return figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a live/src/plotting.py\n",
    "\n",
    "def make_waveform():\n",
    "    waveform, sample_rate = get_latest_file()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = np.arange(0, num_frames) / sample_rate\n",
    "    with plt.xkcd():\n",
    "        figure = Figure() \n",
    "        axes = figure.subplots(num_channels, 1)\n",
    "        if num_channels == 1:\n",
    "            axes = [axes]\n",
    "        for c in range(num_channels):\n",
    "            axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
    "            axes[c].grid(True)\n",
    "            if num_channels > 1:\n",
    "                axes[c].set_ylabel(f\"Channel {c+1}\")\n",
    "        figure.suptitle(\"Waveform\")\n",
    "    return figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(result.json()['outputs'][0]['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Audio Effects as a Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the perspective of an audio programmer, audio effects are software tools designed to modify and enhance sound in various ways. Think of them as virtual gadgets that manipulate audio signals. These effects can range from simple ones, like adjusting volume or adding echo, to complex ones, like simulating guitar pedals or creating futuristic sci-fi sounds. For programmers, creating these effects involves using algorithms and coding skills to transform raw audio data into something sonically appealing.\n",
    "\n",
    "From the perspective of a musician, audio effects are like a palette of colors for painting music. They add depth, texture, and emotion to the sound. Musicians use effects to shape their tones, creating everything from the distorted crunch of an electric guitar to the dreamy reverb in a singer's voice. Effects are essential because they allow artists to express their creativity, enhancing the overall musical experience for the listeners.\n",
    "\n",
    "Now, imagine audio effects software and products as a magical toolbox. Musicians are like artists, and these tools are their brushes and paints. With the right combination of effects, they can craft a masterpiece, just like an artist creates a beautiful painting using various colors and brushes. It's all about transforming ordinary sounds into something extraordinary, adding layers of richness to the musical canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pedalboard import Pedalboard, Distortion, Delay, Reverb, Chorus, Gain, PitchShift, Compressor, Mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay_and_pitch_shift = Pedalboard([\n",
    "    Delay(delay_seconds=0.35, mix=0.8), PitchShift(semitones=9), Gain(gain_db=-4),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_values[0].numpy(), rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_audio = delay_and_pitch_shift(audio_values[0].numpy(), sample_rate=sampling_rate)\n",
    "Audio(new_audio, rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile live/servers/pedal_board/audio_mixer.py\n",
    "\n",
    "from pedalboard import Pedalboard, Distortion, Delay, Reverb, Chorus, Gain, PitchShift, Compressor, Mix\n",
    "from pedalboard.io import AudioFile\n",
    "\n",
    "from mlserver.codecs import decode_args\n",
    "from mlserver import MLModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a live/servers/pedal_board/audio_mixer.py\n",
    "\n",
    "class AudioMixer(MLModel):\n",
    "    async def load(self):\n",
    "        self.delay_and_pitch_shift = Pedalboard([\n",
    "            Delay(delay_seconds=0.25, mix=1.0), PitchShift(semitones=7), Gain(gain_db=-3),\n",
    "        ])\n",
    "\n",
    "    @decode_args\n",
    "    async def predict(self, song: np.ndarray, sample_rate: np.ndarray) -> np.ndarray:\n",
    "\n",
    "        self.passthrough = Gain(gain_db=1)\n",
    "        self.board = Pedalboard([\n",
    "            Compressor(),\n",
    "            Mix([self.passthrough, self.delay_and_pitch_shift]),\n",
    "            Reverb()\n",
    "        ])\n",
    "        self.new_audio = self.board(song, sample_rate=sample_rate[0][0])\n",
    "        return self.new_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile live/servers/pedal_board/model-settings.json\n",
    "{\n",
    "    \"name\": \"novice_dj\",\n",
    "    \"implementation\": \"audio_mixer.AudioMixer\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile live/servers/pedal_board/settings.json\n",
    "{\n",
    "    \"http_port\": 7050,\n",
    "    \"grpc_port\": 7060,\n",
    "    \"metrics_port\": 6050\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_values[0].numpy().shape, sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_music = {\n",
    "    \"inputs\": [\n",
    "        NumpyCodec.encode_input(name='song', payload=audio_values[0].numpy()).dict(),\n",
    "        NumpyCodec.encode_input(name='sample_rate', payload=np.array([sampling_rate])).dict()\n",
    "    ]\n",
    "}\n",
    "new_music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\"http://localhost:7050/v2/models/novice_dj/infer\", json=new_music)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_values[0].numpy(), rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(response.json()['outputs'][0]['data'], rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Midi Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From an audio programmer's perspective, MIDI (Musical Instrument Digital Interface) representations are a digital language that computers and musical instruments use to communicate. MIDI data contains information about musical notes, like which note was played, how long it was held, and how loud it should be. Programmers use MIDI to control virtual instruments and audio software, allowing musicians to create music on computers. It's like a set of instructions that tell the computer exactly what notes to play and how to play them.\n",
    "\n",
    "From a musician's perspective, MIDI representations serve as a bridge between creativity and technology. Musicians can compose intricate pieces without needing to play every instrument physically. MIDI allows them to input musical ideas into computers or synthesizers, which then play back the notes with different sounds. It's incredibly useful for composing, arranging, and experimenting with various musical ideas, even if you don't have a band or an orchestra at your disposal.\n",
    "\n",
    "Imagine MIDI as musical sheet music for the digital age. Musicians write their musical ideas on this digital paper, and when played back through software or instruments, it's like an orchestra following the sheet music, bringing the composition to life. MIDI empowers musicians to explore endless musical possibilities, much like a versatile toolkit in the hands of a skilled craftsman, helping them create beautiful melodies and harmonies effortlessly.\n",
    "\n",
    "For this section, we will be using Spotify's new tool, [basic pitch](https://github.com/spotify/basic-pitch) \n",
    "to convert musical notes to MIDI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame(src='https://basicpitch.spotify.com/about', width=900, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Front- and Back-End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using gradio as it allows us to build nice graphical user interfaces that we can \n",
    "to prototype our models, pipelines, dashboards, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile live/app.py\n",
    "\n",
    "from src.plotting import make_waveform, make_spectogram\n",
    "from src.helpers import *\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a live/app.py\n",
    "\n",
    "\n",
    "with gr.Blocks(theme='gstaff/xkcd') as demo:\n",
    "    gr.Markdown(\"# Music Generation and Editing App\")\n",
    "    gr.Markdown(\"Second Demo of the Day!\")\n",
    "\n",
    "    with gr.Column():\n",
    "        gr.Markdown(\"# Step 1 - Describe the music you want üòé üé∏ üéπ üéµ\")\n",
    "        with gr.Row(equal_height=True):\n",
    "            with gr.Column(min_width=900):\n",
    "                text = gr.Textbox(\n",
    "                    label=\"Name\", lines=3, interactive=True,\n",
    "                    info=\"Audio Prompt for the kind of song you want your model to produce.\",\n",
    "                    value=\"a fast bachata with violin sounds and few notes from a saxophone\",\n",
    "                    placeholder=\"Type your song description in here.\",\n",
    "                )\n",
    "                make_music   = gr.Button(\"Create Music\")\n",
    "            with gr.Column():\n",
    "                tokens      = gr.Slider(label=\"Max Number of New Tokens\", value=200, minimum=5, maximum=1000, step=1)\n",
    "                guidance    = gr.Slider(label=\"Guidance Scale\", value=3, minimum=1, maximum=50, step=1)\n",
    "                sample_rate = gr.Radio([16000, 32000, 44100], label=\"Sample Rate\", value=32000)\n",
    "        \n",
    "        audio_output = gr.Audio()\n",
    "        make_music.click(fn=make_sound, inputs=[text, guidance, tokens, sample_rate], outputs=audio_output, api_name=\"create_music\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a live/app.py\n",
    "\n",
    "        gr.Markdown()\n",
    "        gr.Markdown(\"# Step 2 - Visualize your creation üìà üëÄ üëå\")\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                create_plots = gr.Button(\"Visualize Waveform\")\n",
    "                plot1 = gr.Plot()\n",
    "                create_plots.click(fn=make_waveform, outputs=plot1)\n",
    "            with gr.Column():\n",
    "                create_plots = gr.Button(\"Visualize Spectogram\")\n",
    "                plot2 = gr.Plot()\n",
    "                create_plots.click(fn=make_spectogram, outputs=plot2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a live/app.py\n",
    "\n",
    "        gr.Markdown()\n",
    "        gr.Markdown(\"# Step 3 - Add Some Effects to it üìº üéß üé∑ üéº\")\n",
    "        with gr.Column():\n",
    "            update_music = gr.Button(\"Update your Music\")\n",
    "            output_video = gr.Video(label=\"Output\", elem_id=\"output-video\")\n",
    "            update_music.click(audio_effect, outputs=[output_video])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a live/app.py\n",
    "\n",
    "        gr.Markdown()\n",
    "        gr.Markdown(\"# Step 4 - Create a MIDI Representation! üéõÔ∏è üé∂ üéº\")\n",
    "        gr.HTML(value=\"\"\"<iframe src=\"https://basicpitch.spotify.com/\" height=\"1000\" width=\"100%\"></iframe>\"\"\")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Install the diffusers package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Import the pipeline for AudioLDM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import ____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Come up with a few prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [___, ____, ____]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Download the smallest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"cvssp/audioldm2\"\n",
    "pipe = AudioLDM2Pipeline.from_pretrained(repo_id, torch_dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Create a pipeline and test that it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = pipe(\n",
    "        prompt,\n",
    "        audio_length_in_s=____,\n",
    "        guidance_scale=____,\n",
    "        num_inference_steps=____,\n",
    "        negative_prompt=____,\n",
    "        ...\n",
    ")[\"audios\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Create a server using MLServer and test that it works correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
